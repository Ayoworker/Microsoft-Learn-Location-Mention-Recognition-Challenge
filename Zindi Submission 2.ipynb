{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c517c53f",
   "metadata": {},
   "source": [
    "# Location mention recognision program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662971f",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0173685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb34f93",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6e1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Train_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192e78b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Use a heatmap to visualize missing data\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.title(\"Missing Data Heatmap\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c8a3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id    0\n",
      "text        0\n",
      "location    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f92b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11849\n"
     ]
    }
   ],
   "source": [
    "print(df[\"tweet_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d015e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a1ce078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        flash floods struck a maryland city on sunday,...\n",
      "2        state of emergency declared for maryland flood...\n",
      "3        other parts of maryland also saw significant d...\n",
      "4        catastrophic flooding slams ellicott city, mar...\n",
      "5        watch 1 missing after flash devastates ellicot...\n",
      "                               ...                        \n",
      "73066    mexico city at least a thousand buildings dama...\n",
      "73068    rescue workers recover the body of the last pe...\n",
      "73069    donate from facebook to mexico earthquake reli...\n",
      "73070    we are helping our clients in mexico recover f...\n",
      "73071    thanks for the support! like them you can dona...\n",
      "Name: text, Length: 11849, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # 1. Lowercase the text\n",
    "    text = text.lower()\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 3. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 4. Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # 5. Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # 6. Remove special characters, punctuation, and numbers (including \":\" and \"@\")\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Keeps only alphanumeric characters, spaces, and punctuation\n",
    "    # 7. Remove any standalone punctuation like ':' or '@'\n",
    "    text = re.sub(r'[:@]', '', text)\n",
    "    # 8. Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the microblogs\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "print(df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4a4f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Step 1: Function to create BIO labels with subword handling\n",
    "def create_bio_labels(text, location):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Initialize labels as 'O' (non-location)\n",
    "    bio_labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Tokenize the location for matching\n",
    "    location_tokens = tokenizer.tokenize(location)\n",
    "\n",
    "    # Try to find the location tokens in the text tokens\n",
    "    for i in range(len(tokens) - len(location_tokens) + 1):\n",
    "        if tokens[i:i + len(location_tokens)] == location_tokens:\n",
    "            # Mark the first token of the location as B-LOC and the rest as I-LOC\n",
    "            bio_labels[i] = 'B-LOC'\n",
    "            for j in range(1, len(location_tokens)):\n",
    "                bio_labels[i + j] = 'I-LOC'\n",
    "    \n",
    "    return tokens, bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83485195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle subword tokenization and label adjustment\n",
    "def tokenize_and_align_labels(text, location, tokenizer, label_encoder):\n",
    "    tokens, bio_labels = create_bio_labels(text, location)\n",
    "    \n",
    "    input_ids = []\n",
    "    label_ids = []\n",
    "\n",
    "    for word, label in zip(tokens, bio_labels):\n",
    "        # Tokenize the word (it may break into multiple subwords)\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        \n",
    "        # Convert the tokens to input IDs\n",
    "        word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "        \n",
    "        # Append the input IDs\n",
    "        input_ids.extend(word_ids)\n",
    "        \n",
    "        # Adjust labels: the first subword gets the original label, others get 'I-LOC' or 'O'\n",
    "        if label == 'B-LOC':  # Location start\n",
    "            subword_labels = [label] + ['I-LOC'] * (len(word_tokens) - 1)\n",
    "        else:\n",
    "            subword_labels = [label] + ['O'] * (len(word_tokens) - 1)\n",
    "        \n",
    "        # Transform BIO string labels to numerical labels using the label encoder\n",
    "        label_ids.extend(label_encoder.transform(subword_labels))\n",
    "\n",
    "    return input_ids, label_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3631ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset and aligning tokens with BIO labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 11849/11849 [00:30<00:00, 382.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize and fit the LabelEncoder with the BIO labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['O', 'B-LOC', 'I-LOC'])  # Fit the label encoder with the BIO tags\n",
    "\n",
    "# Step 4: Apply the tokenization and label alignment to each tweet\n",
    "all_input_ids = []\n",
    "all_label_ids = []\n",
    "\n",
    "print(\"Processing dataset and aligning tokens with BIO labels...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    input_ids, label_ids = tokenize_and_align_labels(row['text'], row['location'], tokenizer, label_encoder)\n",
    "    all_input_ids.append(input_ids)\n",
    "    all_label_ids.append(label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfbfb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding tokens and BIO labels...\n",
      "Example input IDs (before padding): [5956, 14295, 4930, 1037, 5374, 2103, 2006, 4465, 1010, 12699, 2041, 4534, 1998, 15021, 3765, 2066, 7198, 10899, 1012]\n",
      "Example label IDs (before padding): [2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Input IDs shape: torch.Size([11849, 50])\n",
      "BIO Labels shape: torch.Size([11849, 50])\n",
      "Attention Masks shape: torch.Size([11849, 50])\n",
      "\n",
      "Original tokens: ['flash', 'floods', 'struck', 'a', 'maryland', 'city', 'on', 'sunday', ',', 'washing', 'out', 'streets', 'and', 'tossing', 'cars', 'like', 'bath', 'toys', '.']\n",
      "Original BIO labels: [2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Input IDs: tensor([ 5956, 14295,  4930,  1037,  5374,  2103,  2006,  4465,  1010, 12699,\n",
      "         2041,  4534,  1998, 15021,  3765,  2066,  7198, 10899,  1012,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "BIO label IDs: tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2])\n",
      "Attention Mask: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Padding and converting to tensors\n",
    "max_len = 50  # Maximum sequence length\n",
    "\n",
    "# Pad tokens and labels\n",
    "def pad_sequences(sequences, max_len, pad_value=0):\n",
    "    padded_seqs = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < max_len:\n",
    "            padded_seq = seq + [pad_value] * (max_len - len(seq))\n",
    "        else:\n",
    "            padded_seq = seq[:max_len]\n",
    "        padded_seqs.append(padded_seq)\n",
    "    return padded_seqs\n",
    "\n",
    "print(\"Padding tokens and BIO labels...\")\n",
    "\n",
    "# Check that all_input_ids and all_label_ids contain numerical values\n",
    "print(f\"Example input IDs (before padding): {all_input_ids[0]}\")\n",
    "print(f\"Example label IDs (before padding): {all_label_ids[0]}\")\n",
    "\n",
    "# Pad input IDs and labels to max_len\n",
    "input_ids_padded = pad_sequences(all_input_ids, max_len, pad_value=tokenizer.pad_token_id)\n",
    "bio_labels_padded = pad_sequences(all_label_ids, max_len, pad_value=label_encoder.transform(['O'])[0])\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_tensor = torch.tensor(input_ids_padded, dtype=torch.long)\n",
    "bio_labels_tensor = torch.tensor(bio_labels_padded, dtype=torch.long)\n",
    "\n",
    "# Create attention masks (1 for real tokens, 0 for padding)\n",
    "attention_masks = [[float(i != tokenizer.pad_token_id) for i in seq] for seq in input_ids_padded]\n",
    "attention_masks_tensor = torch.tensor(attention_masks, dtype=torch.float)\n",
    "\n",
    "# Final output: input_ids_tensor, bio_labels_tensor, attention_masks_tensor\n",
    "print(f\"Input IDs shape: {input_ids_tensor.shape}\")\n",
    "print(f\"BIO Labels shape: {bio_labels_tensor.shape}\")\n",
    "print(f\"Attention Masks shape: {attention_masks_tensor.shape}\")\n",
    "\n",
    "# Example of tokenization and BIO labeling\n",
    "for i, tokens in enumerate(all_input_ids[:1]):\n",
    "    print(\"\\nOriginal tokens:\", tokenizer.convert_ids_to_tokens(tokens))\n",
    "    print(\"Original BIO labels:\", all_label_ids[i])\n",
    "    print(\"Input IDs:\", input_ids_tensor[i])\n",
    "    print(\"BIO label IDs:\", bio_labels_tensor[i])\n",
    "    print(\"Attention Mask:\", attention_masks_tensor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a57fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing sets...\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train/test split with progress bar\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    input_ids_tensor, attention_masks_tensor, bio_labels_tensor, test_size=0.1, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "358927dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Move data to device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_inputs = train_inputs.to(device)\n",
    "test_inputs = test_inputs.to(device)\n",
    "train_masks = train_masks.to(device)\n",
    "test_masks = test_masks.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Step 9: Create dataloaders\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a34aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Initialize the BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 11: Use AdamW optimizer with weight decay for better generalization\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "010c69e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Training function with progress bar\n",
    "def train_model(model, train_dataloader, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fe4c313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|████████████████████████████████████████████████████████████| 334/334 [1:03:18<00:00, 11.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.0756\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|████████████████████████████████████████████████████████████| 334/334 [1:03:34<00:00, 11.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.0270\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|████████████████████████████████████████████████████████████| 334/334 [1:03:15<00:00, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 13: Fine-tuning the BERT model\n",
    "train_model(model, train_dataloader, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80301ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Prediction and converting logits to labels with progress bar\n",
    "def convert_logits_to_labels_batch(predictions, label_encoder, input_ids, attention_masks, batch_size=32):\n",
    "    predicted_labels = []\n",
    "    num_batches = int(np.ceil(len(predictions) / batch_size))\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Converting logits to labels\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(predictions))\n",
    "        \n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "        batch_input_ids = input_ids[start_idx:end_idx]\n",
    "        batch_attention_masks = attention_masks[start_idx:end_idx]\n",
    "        \n",
    "        softmax_predictions = torch.nn.functional.softmax(torch.tensor(batch_predictions), dim=2).numpy()\n",
    "        predicted_indices = np.argmax(softmax_predictions, axis=2)\n",
    "        \n",
    "        batch_predicted_labels = []\n",
    "        for idx, (pred_labels, input_id, attention_mask) in enumerate(zip(predicted_indices, batch_input_ids, batch_attention_masks)):\n",
    "            # Cleaning and transforming predicted labels back to their original string labels\n",
    "            cleaned_labels = [label_encoder.inverse_transform([label])[0] for label, mask in zip(pred_labels, attention_mask) if mask == 1]\n",
    "            batch_predicted_labels.append(cleaned_labels)\n",
    "        \n",
    "        predicted_labels.extend(batch_predicted_labels)\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f65fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Save the trained model, tokenizer, and label encoder\n",
    "output_dir = \"./model_save/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "with open(os.path.join(output_dir, 'label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa81e40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 38/38 [02:15<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating evaluation metrics...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.80      0.91      0.85       785\n",
      "       I-LOC       0.63      0.83      0.72       375\n",
      "           O       1.00      0.99      0.99     31117\n",
      "\n",
      "    accuracy                           0.98     32277\n",
      "   macro avg       0.81      0.91      0.85     32277\n",
      "weighted avg       0.99      0.98      0.99     32277\n",
      "\n",
      "Accuracy: 0.9849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Step 16: Function for model evaluation on the test dataset\n",
    "def evaluate_model(model, test_dataloader, label_encoder):\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for faster evaluation\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Get model outputs (logits)\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Convert logits to predicted labels\n",
    "            predictions = torch.argmax(logits, dim=2).cpu().numpy()\n",
    "\n",
    "            # Remove padding and gather true labels and predictions\n",
    "            batch_labels = batch_labels.cpu().numpy()\n",
    "            batch_attention_masks = batch_attention_masks.cpu().numpy()\n",
    "\n",
    "            for pred, true, mask in zip(predictions, batch_labels, batch_attention_masks):\n",
    "                true_labels = true[mask == 1]  # Ignore padding tokens\n",
    "                predicted_labels = pred[mask == 1]  # Ignore padding tokens\n",
    "                all_true_labels.extend(true_labels)\n",
    "                all_predictions.extend(predicted_labels)\n",
    "\n",
    "    # Convert numeric labels back to string labels\n",
    "    all_true_labels = label_encoder.inverse_transform(all_true_labels)\n",
    "    all_predictions = label_encoder.inverse_transform(all_predictions)\n",
    "\n",
    "    return all_true_labels, all_predictions\n",
    "\n",
    "# Step 17: Evaluate the model\n",
    "true_labels, predicted_labels = evaluate_model(model, test_dataloader, label_encoder)\n",
    "\n",
    "# Step 18: Generate the classification report\n",
    "print(\"Generating evaluation metrics...\")\n",
    "\n",
    "# Flatten the labels to get rid of nested structure and print the classification report\n",
    "print(classification_report(true_labels, predicted_labels, labels=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# Accuracy score\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4721c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load the pre-trained model, tokenizer, and label encoder\n",
    "output_dir = \"./model_save/\"\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "with open(os.path.join(output_dir, 'label_encoder.pkl'), 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82d830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Preprocess the new tweets\n",
    "def preprocess_tweets(new_tweets, tokenizer, max_len=50):\n",
    "    all_tokens = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for tweet in new_tweets:\n",
    "        # Tokenize the tweet\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']  # Add special tokens\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # Create attention masks (1 for actual tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Pad tokens and attention mask to max_len\n",
    "        input_ids = input_ids[:max_len] + [tokenizer.pad_token_id] * (max_len - len(input_ids))\n",
    "        attention_mask = attention_mask[:max_len] + [0] * (max_len - len(attention_mask))\n",
    "\n",
    "        all_tokens.append(input_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return torch.tensor(all_tokens), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b69e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Predict the BIO labels for new tweets\n",
    "def predict_bio_labels(model, tokenizer, new_tweets, label_encoder, batch_size=32):\n",
    "    input_ids, attention_masks = preprocess_tweets(new_tweets, tokenizer)\n",
    "    input_ids, attention_masks = input_ids.to(model.device), attention_masks.to(model.device)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(input_ids), batch_size), desc=\"Predicting BIO labels\"):\n",
    "            batch_input_ids = input_ids[i:i + batch_size]\n",
    "            batch_attention_masks = attention_masks[i:i + batch_size]\n",
    "\n",
    "            # Get model predictions (logits)\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Convert logits to predicted label IDs\n",
    "            batch_predictions = torch.argmax(logits, dim=2).cpu().numpy()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "    # Convert predicted label IDs to BIO labels\n",
    "    predicted_bio_labels = []\n",
    "    for pred in predictions:\n",
    "        predicted_labels = label_encoder.inverse_transform(pred)\n",
    "        predicted_bio_labels.append(predicted_labels)\n",
    "\n",
    "    return predicted_bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01413e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract locations from predicted BIO labels\n",
    "def extract_locations(new_tweets, predicted_bio_labels):\n",
    "    extracted_locations = []\n",
    "    for tweet, bio_labels in zip(new_tweets, predicted_bio_labels):\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        locations = []\n",
    "        current_location = None\n",
    "\n",
    "        for token, label in zip(tokens, bio_labels):\n",
    "            if label.startswith(\"B-LOC\"):  # Start of a location\n",
    "                current_location = token\n",
    "            elif label.startswith(\"I-LOC\") and current_location:  # Continuation of the location\n",
    "                current_location += \" \" + token\n",
    "            else:\n",
    "                if current_location:\n",
    "                    locations.append(current_location)  # Add location to list\n",
    "                    current_location = None\n",
    "\n",
    "        if current_location:\n",
    "            locations.append(current_location)\n",
    "\n",
    "        extracted_locations.append(locations)\n",
    "\n",
    "    return extracted_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "153af870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       what is happening to the infrastructure in new...\n",
      "1       solder missing in flood.. pray for eddison her...\n",
      "2       rt police searching for missing person after d...\n",
      "3       flash flood tears through maryland town for se...\n",
      "4       ellicott city pictures maryland governor decla...\n",
      "                              ...                        \n",
      "2937    he wants to donate some food to homeless cats ...\n",
      "2938    i live the mexico earthquake and my house is d...\n",
      "2939    rt watch national taco day in calgary to benef...\n",
      "2940    oaxaca chiapas mexicos poorest states harshest...\n",
      "2941    rt carlos santana donates 100k to mexico earth...\n",
      "Name: text, Length: 2942, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 5: Read new tweets from a CSV file\n",
    "\n",
    "data = pd.read_csv('Test.csv')\n",
    "\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # 1. Lowercase the text\n",
    "    text = text.lower()\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # 3. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 4. Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # 5. Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # 6. Remove special characters, punctuation, and numbers (including \":\" and \"@\")\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Keeps only alphanumeric characters, spaces, and punctuation\n",
    "    # 7. Remove any standalone punctuation like ':' or '@'\n",
    "    text = re.sub(r'[:@]', '', text)\n",
    "    # 8. Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the microblogs\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "print(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef2dab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting locations from new tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting BIO labels: 100%|███████████████████████████████████████████████████████████| 92/92 [05:38<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming the tweets are in a column named 'text'\n",
    "new_tweets = data['text'].tolist()\n",
    "\n",
    "# Step 6: Run the model on the new tweets\n",
    "print(\"Extracting locations from new tweets...\")\n",
    "predicted_bio_labels = predict_bio_labels(model, tokenizer, new_tweets, label_encoder)\n",
    "extracted_locations = extract_locations(new_tweets, predicted_bio_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915be7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25266afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted locations saved to submission2.csv\n",
      "Tweet: what is happening to the infrastructure in new england? it isnt global warming, its misappropriated funds being abused that shouldve been used maintaining their infrastructure that couldve protected them from floods! like new orleans. their mayor went to ὄ7\n",
      "Extracted Locations: orleans\n",
      "\n",
      "Tweet: solder missing in flood.. pray for eddison hermond! pray for ellicott city, maryland!\n",
      "Extracted Locations: None\n",
      "\n",
      "Tweet: rt police searching for missing person after devastating 1,000year flood in ellicott city, maryland\n",
      "Extracted Locations: None\n",
      "\n",
      "Tweet: flash flood tears through maryland town for second time in two years less than two years after what had been called a once in a 1,000 years flood in 2016, ellicott city, md., sees its historic downtown ravaged anew. one man remains missing. from flas\n",
      "Extracted Locations: None\n",
      "\n",
      "Tweet: ellicott city pictures maryland governor declares state of emergency after severe flash\n",
      "Extracted Locations: governor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Add extracted locations to a new dataframe with only tweet_id and extracted locations\n",
    "df_output = data[['tweet_id']].copy()  # Create a new dataframe with only the tweet_id column\n",
    "\n",
    "# Convert list of extracted locations to a comma-separated string, and write \"None\" if empty\n",
    "df_output['locations'] = ['; '.join(locations) if locations else 'None' for locations in extracted_locations]\n",
    "\n",
    "# Save the results to a new CSV file with only the tweet_id and extracted_locations\n",
    "output_csv_file = 'submission2.csv'\n",
    "df_output.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"Extracted locations saved to {output_csv_file}\")\n",
    "\n",
    "# Print a few results as an example\n",
    "for tweet, locations in zip(new_tweets[:5], extracted_locations[:5]):\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Extracted Locations: {', '.join(locations) if locations else 'None'}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c178a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
